{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779d31f0-150c-4426-b71c-5273aa0f439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5a72d3-f9ff-4bb5-a423-a30c60cf2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/das/work/local/Gits/2023/clembench')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e70b4a-1467-4204-b639-faffeb21452c",
   "metadata": {},
   "source": [
    "# Prototyping Dialogue Games with and for `clemgame`\n",
    "\n",
    "This notebook demonstrates how the package can be used to prototype a game: play around with prompts, define the MOVE RULEs and the GAME RULEs (see below), and set up the game loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99adfa-965e-4ee5-b1d1-e7c8d5a139db",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dddd346-9ac4-4873-bca7-2ae19eba64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'torch'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot load 'backends.huggingface_local_api'. Please make sure that the file exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded backends: anthropic,openai,alephalpha\n"
     ]
    }
   ],
   "source": [
    "from backends import lookup_by_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5ba938-bbdf-4215-a753-e6e0b6bf3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "THIS_MODEL = 'gpt-3.5-turbo-1106'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc8336a-cce8-4842-ad95-5341d73968d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "THIS_MODEL = 'gpt-4-1106-preview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6277d1b5-c211-4cb5-8719-ea9a0f05a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmm = lookup_by_model_name(THIS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af3f3308-5f44-4745-9bbf-c45d16d261c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmm.temperature = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7cda6a1-7462-4652-b5ef-d477c3507402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mlmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       ":param messages: for example\n",
       "        [\n",
       "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
       "            {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
       "            {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
       "        ]\n",
       ":param model: chat-gpt for chat-completion, otherwise text completion\n",
       ":return: the continuation\n",
       "\u001b[0;31mFile:\u001b[0m      ~/work/local/Gits/2023/clembench/backends/openai_api.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?lmm.generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2566b431-0a22-4c3d-b74a-7164988e40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(context, msg, role='user', sysmsg = 'You are a helpful assistant'):\n",
    "    if context == []:\n",
    "        context = [\n",
    "            {\"role\": \"system\", \"content\": sysmsg}\n",
    "        ]\n",
    "    context.append({\"role\": role, \"content\": msg})\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d140c6fc-ecfe-4d7a-8f59-67d2b224f58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant'},\n",
       " {'role': 'user', 'content': 'hello, how are you?'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message([], \"hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237f33e-42a1-40bc-aaa8-8f0ea832b686",
   "metadata": {},
   "source": [
    "Let us try a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e955b40-6ff1-4795-b855-640fee18247a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Benchmarking is a process used by organizations to measure and compare their performance, processes, products, or services against those of other organizations, often considered to be the best in their class or industry. The goal of benchmarking is to identify areas where improvements can be made, to understand how other organizations achieve high performance, and to use this information to enhance one's own performance.\\n\\nThere are several types of benchmarking, including:\\n\\n1. **Internal Benchmarking**: Comparing different departments, teams, or\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = add_message([], \"What is benchmarking?\")\n",
    "_, resp, resp_text = lmm.generate_response(prompt, THIS_MODEL)\n",
    "resp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a0a5a4-961e-4914-a13e-c8a1430cd42a",
   "metadata": {},
   "source": [
    "## Prototyping a game\n",
    "\n",
    "To set up a game, we need two things. First, we need what we shall call a MOVE RULE. This (kind of) rule defines what *form* a valid game move must take. (Having this form enables us to parse it and to even understand in the first place what the move was. For example, if we were to play chess and we would define that requested moves have to follow the [UCI](https://en.wikipedia.org/wiki/Universal_Chess_Interface) notation, then `e2e4` would validate (because it is in that format), and `Nf3` would not (and neither would the text `I will move my knight to F3` and similar).\n",
    "\n",
    "This is independent of the question of whether `e2e4` is a legal move in the current game state. These kinds of things are defined by the GAME RULEs. These rules determine what effect valid (= well-formed) moves have on the current game state: whether they are legal, and whether they update the game state, and if so, whether they perhaps even update the game into a goal state. We might also want to define other criteria that determine whether the game goes on or not, like a limit on the number of turns the game is played.\n",
    "\n",
    "What is important to keep in mind when setting up a clemgame is that both move and game rules need to be checkable *programmatically* (by the game master that you need to implement). That is, it shouldn't require the kind of \"intelligence\" that you want to test in the first place. Anything that is formally define-able should be ok.\n",
    "\n",
    "Let's try a very simple game. A and B are supposed to have a conversation, but in each of their turns, the first letter must be such that taken together, the sequence of first letters follows the sequence in the alphabet, with a given starting letter. So if that starting letter is 'h', the first utterance start with h, the second with i, and so on. This is the GAME RULE. If any player violates that rule, the game ends as a failure. If however the rule has not been violated for $n$ turns, it's a success. (The MOVE RULE is \"start your utterance with 'I SAY:'\". If the output of the player does not have this form, it is rejected. This can lead to a re-prompt (explaining again the move rule), or it can lead to the game being abandoned.)\n",
    "\n",
    "(This is not a terribly good game, as it does not require the integration of information across very many turns -- you just need to look at the previous turn to know which letter you are supposed to use. We might like to impose the constraint that the turns need to stick to a certain topic -- and indeed we will do so in the prompts below -- but that isn't something that we can test. Or at least not easily, and we won't attempt to here. But the game shall suffice to explain the basic concepts, and demonstrate the process of prototyping a game idea.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bce254-a1f6-4b32-9134-ff9cb1d4457b",
   "metadata": {},
   "source": [
    "#### The initial prompts\n",
    "\n",
    "Let's first try to find a good initial prompt for player A, who kicks things off. (Here it took me a couple of iterations to find the following prompt. We'll use that to continue.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5ada343-abaf-465b-92fd-e59ec5e4ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_A = Template('''Let us play a game. I will tell you a topic and you will give me a short sentence that fits to this topic.\n",
    "But I will also tell you a letter. The sentence that you give me has to start with this letter.\n",
    "After you have answered, I will give you my reply, which will start with the letter following your letter in the alphabet.\n",
    "Then it is your turn again, to produce a reply starting with the letter following that one. And so on. Let's go.\n",
    "Start your utterance with I SAY: and do no produce any other text.\n",
    "The topic is: $topic\n",
    "The letter is: $letter''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e15412-84d4-4704-a4fa-a6e286111e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let us play a game. I will tell you a topic and you will give me a short sentence that fits to this topic.\\nBut I will also tell you a letter. The sentence that you give me has to start with this letter.\\nAfter you have answered, I will give you my reply, which will start with the letter following your letter in the alphabet.\\nThen it is your turn again, to produce a reply starting with the letter following that one. And so on. Let's go.\\nStart your utterance with I SAY: and do no produce any other text.\\nThe topic is: birds\\nThe letter is: h\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = 'birds'\n",
    "letter = 'h'\n",
    "init_prompt_A.substitute(topic=topic, letter=letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6bbf3-dc5f-4950-94bd-5dc2893873c8",
   "metadata": {},
   "source": [
    "Let us see what the model makes of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4714bd29-c9b6-41cc-a3e3-ec039c5850b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I SAY: Hawks have impressive eyesight, perfect for spotting prey from high altitudes.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_A, resp, resp_text = lmm.generate_response(add_message([], init_prompt_A.substitute(topic='birds', letter='h')), THIS_MODEL)\n",
    "resp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b32e91-ce60-403b-9478-8b4b36d36f87",
   "metadata": {},
   "source": [
    "Not too bad! Let's code the MOVE RULE (and use it to get at the text itself):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f1ea0f7-c23c-425a-b40a-62e072feb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'I SAY: '\n",
    "def parse_reply(text, prefix=prefix):\n",
    "    if not text.startswith(prefix):\n",
    "        return False\n",
    "    return text[len(prefix):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcb75b71-fcaf-47c1-a853-fbab797995fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hawks have impressive eyesight, perfect for spotting prey from high altitudes.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_reply(resp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b920f9-6990-4f74-8e6f-d4617320b61b",
   "metadata": {},
   "source": [
    "Now we need the GAME RULE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fcd1aa3-aced-48d2-82f4-9fc4f50e495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_move(text, letter):\n",
    "    token = text.split()\n",
    "    if token[0][0].lower() == letter: # and token[-1][0].lower() == letter:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82a16807-80cd-4efe-9369-34b7372577ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_move(parse_reply(resp_text), letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b18560-d996-4d78-ace0-9510a9c14f02",
   "metadata": {},
   "source": [
    "Alright! Now for player B, who needs a slightly different prompt (as they *continue* the conversation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62fd1601-96e3-482d-a979-997f0884e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_B = Template('''Let us play a game. I will give you a sentence.\n",
    "The first word in my sentence starts with a certain letter.\n",
    "I want you to give me a sentence as a reply, with the same topic as my sentence, but different from my sentence.\n",
    "The first word of your sentence should start with the next letter in the alphabet from the letter my sentence started with.\n",
    "Let us try to have a whole conversation like that.\n",
    "Please start your reply with 'I SAY:' and do not produce any other text.\n",
    "Let's go.\n",
    "My sentence is: $sentence\n",
    "What do you say?''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "447c2691-9dbc-4a8e-a61d-ddcac307da17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let us play a game. I will give you a sentence.\\nThe first word in my sentence starts with a certain letter.\\nI want you to give me a sentence as a reply, with the same topic as my sentence, but different from my sentence.\\nThe first word of your sentence should start with the next letter in the alphabet from the letter my sentence started with.\\nLet us try to have a whole conversation like that.\\nPlease start your reply with 'I SAY:' and do not produce any other text.\\nLet's go.\\nMy sentence is: Hawks have impressive eyesight, perfect for spotting prey from high altitudes.\\nWhat do you say?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = parse_reply(resp_text)\n",
    "prompt_B = init_prompt_B.substitute(sentence=sentence)\n",
    "prompt_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3ddeae6-a8e9-4b59-826d-8884c152d213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I SAY: Intelligent birds, they can adjust their hunting strategies based on environmental cues.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_B, resp_B, resp_B_text = lmm.generate_response(add_message([], prompt_B), THIS_MODEL)\n",
    "resp_B_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cb68a4d-2518-4f5f-951c-f304f9dfb301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intelligent birds, they can adjust their hunting strategies based on environmental cues.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_reply(resp_B_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff68b4-c992-49f8-90fb-0e7bcf73e4d1",
   "metadata": {},
   "source": [
    "Alright! Now we need to check this move, both for well-formedness and against the game rules.\n",
    "\n",
    "Here, it would be helpful to already formalise the game state. (Actually, when you're just playing around with an idea, you can postone this. I've only added this after iterating on this a couple of times to get a feel for what I need to keep track of.)\n",
    "\n",
    "To determine whether a move met the game rule, we need to know what letter it was supposed to start with. If it met the rule, we advance the counter of successful moves, and also advance the expected letter. We also check whether we have reached the maximal number of turns. If so, the game is a win. If however the move did not meet the rule, we end the game as a failure. \n",
    "\n",
    "If the attempted move didn't even validate, we abort the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f87ed8af-3d5a-4398-988b-475e3a21bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialsGameState():\n",
    "    def __init__(self, letter):\n",
    "        self.letter = letter\n",
    "        self.n_moves = 0\n",
    "        self.success = False\n",
    "        self.aborted = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ee1eb-d442-4d90-97d7-e2a1c906cd27",
   "metadata": {},
   "source": [
    "Before A's move, the game state was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "409418a2-813d-4d6d-9bb3-d3adee935eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('h', 0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_game = InitialsGameState('h')\n",
    "this_game.letter, this_game.n_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f04ad4-afdb-4cf5-bec6-7e5740ebf0a8",
   "metadata": {},
   "source": [
    "A's move was successful, so we should increment the letter that we are expecting now. We can define a method for that on the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7421d349-962d-49ac-ac65-0ad5f3ec7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialsGameState():\n",
    "    def __init__(self, letter):\n",
    "        self.letter = letter\n",
    "        self.n_moves = 0\n",
    "        self.success = False\n",
    "        self.aborted = False\n",
    "\n",
    "    def increment_state(self):\n",
    "        self.letter = chr(ord(self.letter) + 1 )\n",
    "        self.n_moves += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4079f4-30fa-4fb2-9d71-c0cbcb93ad77",
   "metadata": {},
   "source": [
    "(Since we've redefined the state class, need to set to before A's move again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c77ce995-0d53-421c-8c9f-de0ee00707d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i', 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_game = InitialsGameState('h')\n",
    "this_game.increment_state()\n",
    "this_game.letter, this_game.n_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bcd1c73-8429-46ce-be89-66ef36481d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_move(parse_reply(resp_B_text), letter=this_game.letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3b08d-47c6-479a-a833-9c9879d77d0a",
   "metadata": {},
   "source": [
    "(By now it should be clear that parsing and validating could be methods of the game state, but we'll skip that for now...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93413dcb-cf80-4e75-ba80-295103820fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_game.increment_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46eb9690-716c-464a-a808-af86cd6c7e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_game.letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef821fc-b9b0-4645-82d0-2bb5492bba9f",
   "metadata": {},
   "source": [
    "#### The Game Loop\n",
    "\n",
    "Not too bad. From here on, the game consists of giving A the turn (with all previous history), parsing and validating the response, then giving B the turn, parsing and validating the response, and so on, breaking the loop if a) an unparseable move was attempted (more correctly: no understandable move was made), or b) a loosing move was made (wrong initial), or c) the max # of turns has been reached. We are not going to implement that here. We can simulate the loop by going back to A's cell below, and break whenver we run out of patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71ae0dd8-1718-4e5c-a84d-d4332973ed92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant'},\n",
       " {'role': 'user',\n",
       "  'content': \"Let us play a game. I will tell you a topic and you will give me a short sentence that fits to this topic.\\nBut I will also tell you a letter. The sentence that you give me has to start with this letter.\\nAfter you have answered, I will give you my reply, which will start with the letter following your letter in the alphabet.\\nThen it is your turn again, to produce a reply starting with the letter following that one. And so on. Let's go.\\nStart your utterance with I SAY: and do no produce any other text.\\nThe topic is: birds\\nThe letter is: h\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I SAY: Hawks have impressive eyesight, perfect for spotting prey from high altitudes.'},\n",
       " {'role': 'user',\n",
       "  'content': 'I SAY: Intelligent birds, they can adjust their hunting strategies based on environmental cues.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_prompt_A = add_message(prompt_A, resp_text, role='assistant')\n",
    "next_prompt_A = add_message(next_prompt_A, resp_B_text, role='user')\n",
    "next_prompt_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a2d20f0-c754-42d1-bfe6-e78a020e3587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I SAY: Jackdaws are known for their playful behavior and intelligence, often seen using tools to obtain food.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_A, resp, resp_text = lmm.generate_response(next_prompt_A, THIS_MODEL)\n",
    "resp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b258394e-e1a9-406b-aa34-2789d6201f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY\n"
     ]
    }
   ],
   "source": [
    "psd_reply = parse_reply(resp_text)\n",
    "if psd_reply:\n",
    "    if check_move(psd_reply, letter=this_game.letter):\n",
    "        print('YAY')\n",
    "    else:\n",
    "        print('LOST')\n",
    "else:\n",
    "    print('NOT WELL-FORMED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59083b-a96d-4d30-90ba-e68e0fb4a9e3",
   "metadata": {},
   "source": [
    "If we get a non well-formed reply, we could add a reprompting loop (for which we'd need to design a prompt) and try for couple of iteration if we can prise one out of the model... Or immediately break out here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a2f63f7-5d2f-4f69-8d19-ca35324d917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_game.increment_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87bd9f1b-24ed-4b3f-9cfa-cc09af6f542b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant'},\n",
       " {'role': 'user',\n",
       "  'content': \"Let us play a game. I will give you a sentence.\\nThe first word in my sentence starts with a certain letter.\\nI want you to give me a sentence as a reply, with the same topic as my sentence, but different from my sentence.\\nThe first word of your sentence should start with the next letter in the alphabet from the letter my sentence started with.\\nLet us try to have a whole conversation like that.\\nPlease start your reply with 'I SAY:' and do not produce any other text.\\nLet's go.\\nMy sentence is: Hawks have impressive eyesight, perfect for spotting prey from high altitudes.\\nWhat do you say?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I SAY: Intelligent birds, they can adjust their hunting strategies based on environmental cues.'},\n",
       " {'role': 'user',\n",
       "  'content': 'I SAY: Jackdaws are known for their playful behavior and intelligence, often seen using tools to obtain food.'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_prompt_B = add_message(prompt_B, resp_B_text, role='assistant')\n",
    "next_prompt_B = add_message(next_prompt_B, resp_text, role='user')\n",
    "next_prompt_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48ca5f50-6e35-4f30-abc5-aca011152c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I SAY: Kestrels, too, exhibit fascinating hunting techniques, like hovering mid-air to spot their next meal on the ground.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_B, resp_B, resp_B_text = lmm.generate_response(next_prompt_B, THIS_MODEL)\n",
    "resp_B_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb03fc11-b056-4c50-b313-5f44c05a9a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY\n"
     ]
    }
   ],
   "source": [
    "psd_reply = parse_reply(resp_B_text)\n",
    "if psd_reply:\n",
    "    if check_move(psd_reply, letter=this_game.letter):\n",
    "        print('YAY')\n",
    "    else:\n",
    "        print('LOST')\n",
    "else:\n",
    "    print('NOT WELL-FORMED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1666e6-660b-4f2b-9bd9-d7a9b965aed1",
   "metadata": {},
   "source": [
    "Ok. If we made it here, we can jump again to the beginning of the loop (see cell above), and just execute the cells again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ffcbe-66e8-4a0f-abdd-3bd68c73f478",
   "metadata": {},
   "source": [
    "Good. Looks like this game has a chance of working with at least one model! (In fact, it looks like this game is too easy, and we'd need to think again about what it really was supposed to show...) \n",
    "\n",
    "To turn this into something that could be run as part of a proper benchmark, we'd now need to do a lot of work wrapping stuff around this, like running it repeated times from different starting letters, logging everything properly, etc. etc.. But fear not! This is what the `clemgame` framework gives you, and what is described in the `howto_add_game_example.ipynb` notebook.\n",
    "\n",
    "Remember: the purpose of this notebook was to show you what the *outcome* of a prototyping session might look like. The actual prototyping will involve trying out various prompts, coming to a better idea of what the game state is and how to represent it, etc. That is, it will look a lot messier. But it might not be a bad idea at the end of your session to clean up what you have, and prepare something that looks more like this notebook. This should form a good basis for then implementing the game properly in the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a37ed-567b-405b-9907-7a40de905bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
