[
  {
    "model_name": "openchat",
    "model_id": "openchat-3.5-0106",
    "backend": "openai_compatible"
  },
  {
    "model_name": "codellama-34b",
    "model_id": "codellama-34b-instruct",
    "backend": "openai_compatible"
  },
  {
    "model_name": "Llama-3-70B-Instruct",
    "model_id": "meta-llama/Meta-Llama-3-70B-Instruct",
    "backend": "openai_compatible"
  },
  {
    "model_name": "Llama-3-70B-Together.ai",
    "model_id": "meta-llama/Llama-3-70b-chat-hf",
    "backend": "openai_compatible"
  },
  {
    "model_name": "Llama-3-8B-Instruct",
    "model_id": "meta-llama/Meta-Llama-3-8B-Instruct",
    "backend": "openai_compatible"
  },
  {
    "model_name": "Mixtral-8x22B-Instruct-v0.1",
    "model_id": "mistralai/Mixtral-8x22B-Instruct-v0.1",
    "backend": "openai_compatible"
  },
  {
    "model_name": "Mixtral-8x7B-Instruct-v0.1",
    "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "backend": "openai_compatible"
  },
  {
    "model_name": "fsc-openchat-3.5-0106",
    "model_id": "openchat-3.5-0106",
    "backend": "openai_compatible"
  },
  {
    "model_name": "fsc-codellama-34b-instruct",
    "model_id": "codellama-34b-instruct",
    "backend": "openai_compatible"
  },
  {
    "model_name": "gpt-4-1106-vision-preview",
    "model_id": "gpt-4-1106-vision-preview",
    "backend": "openai",
    "supports_images": true,
    "support_multiple_images": true
  },
  {
    "model_name": "gpt-4o-2024-05-13",
    "model_id": "gpt-4o-2024-05-13",
    "backend": "openai",
    "supports_images": true,
    "support_multiple_images": true
  },
  {
    "model_name": "gpt-4-turbo-2024-04-09",
    "model_id": "gpt-4-turbo-2024-04-09",
    "backend": "openai"
  },
  {
    "model_name": "gpt-4-1106-preview",
    "model_id": "gpt-4-1106-preview",
    "backend": "openai"
  },
  {
    "model_name": "gpt-4-0125-preview",
    "model_id": "gpt-4-0125-preview",
    "backend": "openai"
  },
  {
    "model_name": "gpt-3.5-turbo-0125",
    "model_id": "gpt-3.5-turbo-0125",
    "backend": "openai"
  },
  {
    "model_name": "gpt-4-0613",
    "model_id": "gpt-4-0613",
    "backend": "openai"
  },
  {
    "model_name": "gpt-4-0314",
    "model_id": "gpt-4-0314",
    "backend": "openai"
  },
  {
    "model_name": "gpt-3.5-turbo-1106",
    "model_id": "gpt-3.5-turbo-1106",
    "backend": "openai"
  },
  {
    "model_name": "gpt-3.5-turbo-0613",
    "model_id": "gpt-3.5-turbo-0613",
    "backend": "openai"
  },
  {
    "model_name": "mistral-medium-2312",
    "model_id": "mistral-medium-2312",
    "backend": "mistral"
  },
  {
    "model_name": "mistral-tiny-2312",
    "model_id": "mistral-tiny-2312",
    "backend": "mistral"
  },
  {
    "model_name": "mistral-small-2312",
    "model_id": "mistral-small-2312",
    "backend": "mistral"
  },
  {
    "model_name": "mistral-large-2402",
    "model_id": "mistral-large-2402",
    "backend": "mistral"
  },
  {
    "model_name": "command",
    "model_id": "command",
    "backend": "cohere"
  },
  {
    "model_name": "command-r",
    "model_id": "command-r",
    "backend": "cohere"
  },
  {
    "model_name": "command-r-plus",
    "model_id": "command-r-plus",
    "backend": "cohere"
  },
  {
    "model_name": "command-light",
    "model_id": "command-light",
    "backend": "cohere"
  },
  {
    "model_name": "claude-v1.3",
    "model_id": "claude-v1.3",
    "backend": "anthropic"
  },
  {
    "model_name": "claude-v1.3-100k",
    "model_id": "claude-v1.3-100k",
    "backend": "anthropic"
  },
  {
    "model_name": "claude-instant-1.2",
    "model_id": "claude-instant-1.2",
    "backend": "anthropic"
  },
  {
    "model_name": "claude-2",
    "model_id": "claude-2",
    "backend": "anthropic"
  },
  {
    "model_name": "claude-2.1",
    "model_id": "claude-2.1",
    "backend": "anthropic"
  },
  {
    "model_name": "claude-3-opus-20240229",
    "model_id": "claude-3-opus-20240229",
    "backend": "anthropic",
    "supports_images": true,
    "support_multiple_images": true
  },
  {
    "model_name": "claude-3-sonnet-20240229",
    "model_id": "claude-3-sonnet-20240229",
    "backend": "anthropic",
    "supports_images": true,
    "support_multiple_images": true
  },
  {
    "model_name": "claude-3-haiku-20240307",
    "model_id": "claude-3-haiku-20240307",
    "backend": "anthropic",
    "supports_images": true,
    "support_multiple_images": true
  },
  {
    "model_name": "gemini-1.0-pro",
    "model_id": "gemini-1.0-pro",
    "backend": "google"
  },
  {
    "model_name": "gemini-1.5-flash-latest",
    "model_id": "gemini-1.5-flash-latest",
    "backend": "google",
    "supports_images": true,
    "support_multiple_images": true
  },
  {
    "model_name": "gemini-1.5-pro-latest",
    "model_id": "gemini-1.5-pro-latest",
    "backend": "google",
    "supports_images": true,
    "support_multiple_images": true
  },
  {
    "model_name": "luminous-supreme-control",
    "model_id": "luminous-supreme-control",
    "backend": "alephalpha"
  },
  {
    "model_name": "luminous-supreme",
    "model_id": "luminous-supreme",
    "backend": "alephalpha"
  },
  {
    "model_name": "luminous-extended",
    "model_id": "luminous-extended",
    "backend": "alephalpha"
  },
  {
    "model_name": "luminous-base",
    "model_id": "luminous-base",
    "backend": "alephalpha"
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.1",
    "backend": "huggingface_local",
    "huggingface_id": "mistralai/Mistral-7B-Instruct-v0.1",
    "premade_chat_template": true,
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "sheep-duck-llama-2-70b-v1.1",
    "backend": "huggingface_local",
    "huggingface_id": "Riiid/sheep-duck-llama-2-70b-v1.1",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ '### User:\\n' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'system' %}{{ '### System:\\n' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ '### Assistant:\\n' + message['content'] + '\\n\\n' }}{% endif %}{% if loop.last %}{{ '### Assistant:\\n' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "sheep-duck-llama-2-13b",
    "backend": "huggingface_local",
    "huggingface_id": "Riiid/sheep-duck-llama-2-13b",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ '### User:\\n' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'system' %}{{ '### System:\\n' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ '### Assistant:\\n' + message['content'] + '\\n\\n' }}{% endif %}{% if loop.last %}{{ '### Assistant:\\n' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "falcon-7b-instruct",
    "backend": "huggingface_local",
    "huggingface_id": "tiiuae/falcon-7b-instruct",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + '\\n' }}{% endif %}{% if loop.last %}{{ 'ASSISTANT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "<|endoftext|>"
  },
  {
    "model_name": "falcon-40b-instruct",
    "backend": "huggingface_local",
    "huggingface_id": "tiiuae/falcon-40b-instruct",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + '\\n' }}{% endif %}{% if loop.last %}{{ 'ASSISTANT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "<|endoftext|>"
  },
  {
    "model_name": "oasst-sft-4-pythia-12b-epoch-3.5",
    "backend": "huggingface_local",
    "huggingface_id": "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|prompter|>' + message['content'] + '<|endoftext|>' }}{% elif message['role'] == 'assistant' %}{{ '<|assistant|>' + message['content'] + '<|endoftext|>' }}{% endif %}{% if loop.last %}{{ '<|assistant|>' }}{% endif %}{% endfor %}",
    "eos_to_cull": "<|endoftext|>"
  },
  {
    "model_name": "koala-13B-HF",
    "backend": "huggingface_local",
    "huggingface_id": "TheBloke/koala-13B-HF",
    "premade_chat_template": false,
    "custom_chat_template": "{{ 'BEGINNING OF CONVERSATION: ' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + ' ' }}{% elif message['role'] == 'assistant' %}{{ 'GPT: ' + message['content'] + ' ' }}{% endif %}{% if loop.last %}{{ 'GPT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "Wizard-Vicuna-13B-Uncensored-HF",
    "backend": "huggingface_local",
    "huggingface_id": "TheBloke/Wizard-Vicuna-13B-Uncensored-HF",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + '</s>\\n' }}{% endif %}{% if loop.last %}{{ 'ASSISTANT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "WizardLM-70b-v1.0",
    "backend": "huggingface_local",
    "huggingface_id": "WizardLM/WizardLM-70b-v1.0",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + '</s>\\n' }}{% endif %}{% if loop.last %}{{ 'ASSISTANT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "WizardLM-13b-v1.2",
    "backend": "huggingface_local",
    "huggingface_id": "WizardLM/WizardLM-13b-v1.2",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + '</s>\\n' }}{% endif %}{% if loop.last %}{{ 'ASSISTANT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "vicuna-7b-v1.5",
    "backend": "huggingface_local",
    "huggingface_id": "lmsys/vicuna-7b-v1.5",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + '</s>\\n' }}{% endif %}{% if loop.last %}{{ 'ASSISTANT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "vicuna-13b-v1.5",
    "backend": "huggingface_local",
    "huggingface_id": "lmsys/vicuna-13b-v1.5",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + '</s>\\n' }}{% endif %}{% if loop.last %}{{ 'ASSISTANT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "vicuna-33b-v1.3",
    "backend": "huggingface_local",
    "huggingface_id": "lmsys/vicuna-33b-v1.3",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + '</s>\\n' }}{% endif %}{% if loop.last %}{{ 'ASSISTANT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "gpt4all-13b-snoozy",
    "backend": "huggingface_local",
    "huggingface_id": "nomic-ai/gpt4all-13b-snoozy",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + '</s>\\n' }}{% endif %}{% if loop.last %}{{ 'ASSISTANT:' }}{% endif %}{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "CodeLlama-34b-Instruct-hf",
    "backend": "huggingface_local",
    "huggingface_id": "codellama/CodeLlama-34b-Instruct-hf",
    "premade_chat_template": true,
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "zephyr-7b-alpha",
    "backend": "huggingface_local",
    "huggingface_id": "HuggingFaceH4/zephyr-7b-alpha",
    "premade_chat_template": true,
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "zephyr-7b-beta",
    "backend": "huggingface_local",
    "huggingface_id": "HuggingFaceH4/zephyr-7b-beta",
    "premade_chat_template": true,
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "openchat_3.5",
    "backend": "huggingface_local",
    "huggingface_id": "openchat/openchat_3.5",
    "premade_chat_template": true,
    "eos_to_cull": "<|end_of_turn|>"
  },
  {
    "model_name": "Yi-34B-Chat",
    "backend": "huggingface_local",
    "huggingface_id": "01-ai/Yi-34B-Chat",
    "premade_chat_template": true,
    "slow_tokenizer": true,
    "output_split_prefix": "assistant\n",
    "eos_to_cull": "<|im_end|>"
  },
  {
    "model_name": "deepseek-llm-7b-chat",
    "backend": "huggingface_local",
    "huggingface_id": "deepseek-ai/deepseek-llm-7b-chat",
    "premade_chat_template": true,
    "eos_to_cull": "<｜end▁of▁sentence｜>"
  },
  {
    "model_name": "deepseek-llm-67b-chat",
    "backend": "huggingface_local",
    "huggingface_id": "deepseek-ai/deepseek-llm-67b-chat",
    "premade_chat_template": true,
    "eos_to_cull": "<｜end▁of▁sentence｜>"
  },
  {
    "model_name": "tulu-2-dpo-7b",
    "backend": "huggingface_local",
    "huggingface_id": "allenai/tulu-2-dpo-7b",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "tulu-2-dpo-70b",
    "backend": "huggingface_local",
    "huggingface_id": "allenai/tulu-2-dpo-70b",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "Mixtral-8x7B-Instruct-v0.1",
    "backend": "huggingface_local",
    "huggingface_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "premade_chat_template": true,
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "SUS-Chat-34B",
    "backend": "huggingface_local",
    "huggingface_id": "SUSTech/SUS-Chat-34B",
    "premade_chat_template": false,
    "custom_chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{ '### Human: ' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ '### Assistant: ' + message['content'] }}{% endif %}{% if loop.last %}{{ '### Assistant: ' }}{% endif %}{% endfor %}",
    "slow_tokenizer": true,
    "eos_to_cull": "<|endoftext|>"
  },
  {
    "model_name": "CodeLlama-70b-Instruct-hf",
    "backend": "huggingface_local",
    "huggingface_id": "codellama/CodeLlama-70b-Instruct-hf",
    "premade_chat_template": true,
    "eos_to_cull": "<step>"
  },
  {
    "model_name": "openchat-3.5-0106",
    "backend": "huggingface_local",
    "huggingface_id": "openchat/openchat-3.5-0106",
    "premade_chat_template": true,
    "eos_to_cull": "<|end_of_turn|>"
  },
  {
    "model_name": "openchat-3.5-1210",
    "backend": "huggingface_local",
    "huggingface_id": "openchat/openchat-3.5-1210",
    "premade_chat_template": true,
    "eos_to_cull": "<|end_of_turn|>"
  },
  {
    "model_name": "Nous-Hermes-2-Mixtral-8x7B-DPO",
    "backend": "huggingface_local",
    "huggingface_id": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
    "premade_chat_template": true,
    "eos_to_cull": "<|im_end|>"
  },
  {
    "model_name": "Smaug-72B-v0.1",
    "backend": "huggingface_local",
    "huggingface_id": "abacusai/Smaug-72B-v0.1",
    "premade_chat_template": false,
    "custom_chat_template": "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\n' + system_message + '\n<</SYS>>\n\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}",
    "eos_to_cull": "<|endoftext|>"
  },
  {
    "model_name": "Smaug-34B-v0.1",
    "backend": "huggingface_local",
    "huggingface_id": "abacusai/Smaug-34B-v0.1",
    "premade_chat_template": false,
    "custom_chat_template": "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\n' + system_message + '\n<</SYS>>\n\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}",
    "eos_to_cull": "<|endoftext|>"
  },
  {
    "model_name": "Qwen1.5-7B-Chat",
    "backend": "huggingface_local",
    "huggingface_id": "Qwen/Qwen1.5-7B-Chat",
    "premade_chat_template": true,
    "eos_to_cull": "<|im_end|>"
  },
  {
    "model_name": "Qwen1.5-72B-Chat",
    "backend": "huggingface_local",
    "huggingface_id": "Qwen/Qwen1.5-72B-Chat",
    "premade_chat_template": true,
    "eos_to_cull": "<|im_end|>"
  },
  {
    "model_name": "Swallow-70b-instruct-v0.1",
    "backend": "huggingface_local",
    "huggingface_id": "tokyotech-llm/Swallow-70b-instruct-v0.1",
    "premade_chat_template": true,
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "Phi-3-mini-128k-instruct",
    "backend": "huggingface_local",
    "huggingface_id": "microsoft/Phi-3-mini-128k-instruct",
    "premade_chat_template": true,
    "eos_to_cull": "<|endoftext|>"
  },
  {
    "model_name": "Starling-LM-7B-beta",
    "backend": "huggingface_local",
    "huggingface_id": "Nexusflow/Starling-LM-7B-beta",
    "premade_chat_template": true,
    "eos_to_cull": "<|end_of_turn|>"
  },
  {
    "model_name": "llama-2-7b-chat-hf",
    "backend": "huggingface_local",
    "requires_api_key": true,
    "huggingface_id": "meta-llama/llama-2-7b-chat-hf",
    "premade_chat_template": true,
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "llama-2-13b-chat-hf",
    "backend": "huggingface_local",
    "requires_api_key": true,
    "huggingface_id": "meta-llama/llama-2-13b-chat-hf",
    "premade_chat_template": true,
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "llama-2-70b-chat-hf",
    "backend": "huggingface_local",
    "requires_api_key": true,
    "huggingface_id": "meta-llama/llama-2-70b-chat-hf",
    "premade_chat_template": true,
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "gemma-7b-it",
    "backend": "huggingface_local",
    "requires_api_key": true,
    "huggingface_id": "google/gemma-7b-it",
    "premade_chat_template": true,
    "eos_to_cull": "<eos>"
  },
  {
    "model_name": "gemma-1.1-2b-it",
    "backend": "huggingface_local",
    "requires_api_key": true,
    "huggingface_id": "google/gemma-1.1-2b-it",
    "premade_chat_template": true,
    "eos_to_cull": "<eos>"
  },
  {
    "model_name": "gemma-1.1-7b-it",
    "backend": "huggingface_local",
    "requires_api_key": true,
    "huggingface_id": "google/gemma-1.1-7b-it",
    "premade_chat_template": true,
    "eos_to_cull": "<eos>"
  },
  {
    "model_name": "codegemma-7b-it",
    "backend": "huggingface_local",
    "requires_api_key": true,
    "huggingface_id": "google/codegemma-7b-it",
    "premade_chat_template": true,
    "eos_to_cull": "<eos>"
  },
  {
    "model_name": "recurrentgemma-2b-it",
    "backend": "huggingface_local",
    "requires_api_key": true,
    "huggingface_id": "google/recurrentgemma-2b-it",
    "premade_chat_template": true,
    "eos_to_cull": "<eos>"
  },
  {
    "model_name": "Qwen1.5-0.5B-Chat-GGUF-q8",
    "backend": "llamacpp",
    "huggingface_id": "Qwen/Qwen1.5-0.5B-Chat-GGUF",
    "filename": "*q8_0.gguf",
    "premade_chat_template": true,
    "bos_string": "<s>",
    "eos_string": "<|im_end|>",
    "eos_to_cull": "<|im_end|>"
  },
  {
    "model_name": "CapybaraHermes-2.5-Mistral-7B-GGUF-q4",
    "backend": "llamacpp",
    "huggingface_id": "TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF",
    "filename": "*Q4_0.gguf",
    "premade_chat_template": true,
    "bos_string": "<s>",
    "eos_string": "<|im_end|>",
    "eos_to_cull": "<|im_end|>"
  },
  {
    "model_name": "CapybaraHermes-2.5-Mistral-7B-GGUF-q5",
    "backend": "llamacpp",
    "huggingface_id": "TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF",
    "filename": "*Q5_0.gguf",
    "premade_chat_template": true,
    "bos_string": "<s>",
    "eos_string": "<|im_end|>",
    "eos_to_cull": "<|im_end|>"
  },
  {
    "model_name": "CapybaraHermes-2.5-Mistral-7B-GGUF-q5-k-s",
    "backend": "llamacpp",
    "huggingface_id": "TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF",
    "filename": "*Q5_K_S.gguf",
    "premade_chat_template": true,
    "bos_string": "<s>",
    "eos_string": "<|im_end|>",
    "eos_to_cull": "<|im_end|>"
  },
  {
    "model_name": "EstopianMaid-13B-GGUF-q2-k",
    "backend": "llamacpp",
    "huggingface_id": "TheBloke/EstopianMaid-13B-GGUF",
    "filename": "*Q2_K.gguf",
    "premade_chat_template": false,
    "custom_chat_template": "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'].strip() + '\\n\\n' %}{% else %}{% set loop_messages = messages %}{% set system_message = '' %}{% endif %}{% if system_message %}{{ bos_token + system_message }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{bos_token + '### Instruction:\\n' + message['content'].strip() + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ '### Response:\\n' + message['content'].strip() + eos_token + '\\n\\n' }}{% endif %}{% if loop.last and message['role'] == 'user' and add_generation_prompt %}{{ '### Response:\\n' }}{% endif %}{% endfor %}",
    "bos_string": "<s>",
    "eos_string": "</s>",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "EstopianMaid-13B-GGUF-q3-k-s",
    "backend": "llamacpp",
    "huggingface_id": "TheBloke/EstopianMaid-13B-GGUF",
    "filename": "*Q3_K_S.gguf",
    "premade_chat_template": false,
    "custom_chat_template": "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'].strip() + '\\n\\n' %}{% else %}{% set loop_messages = messages %}{% set system_message = '' %}{% endif %}{% if system_message %}{{ bos_token + system_message }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{bos_token + '### Instruction:\\n' + message['content'].strip() + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ '### Response:\\n' + message['content'].strip() + eos_token + '\\n\\n' }}{% endif %}{% if loop.last and message['role'] == 'user' and add_generation_prompt %}{{ '### Response:\\n' }}{% endif %}{% endfor %}",
    "bos_string": "<s>",
    "eos_string": "</s>",
    "eos_to_cull": "</s>"
  },
  {
    "model_name": "openchat_3.5-GGUF-q5",
    "backend": "llamacpp",
    "huggingface_id": "TheBloke/openchat_3.5-GGUF",
    "filename": "*Q5_0.gguf",
    "premade_chat_template": false,
    "custom_chat_template": "{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}",
    "bos_string": "<s>",
    "eos_string": "<|end_of_turn|>",
    "eos_to_cull": "<|end_of_turn|>"
  },
  {
    "model_name": "Meta-Llama-3-70B-Instruct-GGUF-q4",
    "backend": "llamacpp",
    "huggingface_id": "MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF",
    "filename": "*Q4_K_M.gguf",
    "premade_chat_template": true,
    "bos_string": "<|begin_of_text|>",
    "eos_string": "<|eot_id|>",
    "eos_to_cull": "<|eot_id|>"
  },
  {
    "model_name": "Meta-Llama-3-70B-Instruct-GGUF-q8",
    "backend": "llamacpp",
    "huggingface_id": "MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF",
    "filename": "*Q8_0-00001-of-00002.gguf",
    "additional_files": ["*Q8_0-00002-of-00002.gguf"],
    "premade_chat_template": true,
    "bos_string": "<|begin_of_text|>",
    "eos_string": "<|eot_id|>",
    "eos_to_cull": "<|eot_id|>"
  },
  {
    "model_name": "c4ai-command-r-plus-GGUF-q4",
    "backend": "llamacpp",
    "huggingface_id": "pmysl/c4ai-command-r-plus-GGUF",
    "filename": "*Q4_K_M-00001-of-00002.gguf",
    "additional_files": ["*Q4_K_M-00002-of-00002.gguf"],
    "premade_chat_template": true,
    "bos_string": "<BOS_TOKEN>",
    "eos_string": "<|END_OF_TURN_TOKEN|>",
    "eos_to_cull": "<|END_OF_TURN_TOKEN|>"
  },
  {
    "model_name": "c4ai-command-r-plus-GGUF-q8",
    "backend": "llamacpp",
    "huggingface_id": "pmysl/c4ai-command-r-plus-GGUF",
    "filename": "*Q8_0-00001-of-00003.gguf",
    "additional_files": ["*Q8_0-00002-of-00003.gguf", "*Q8_0-00003-of-00003.gguf"],
    "premade_chat_template": true,
    "bos_string": "<BOS_TOKEN>",
    "eos_string": "<|END_OF_TURN_TOKEN|>",
    "eos_to_cull": "<|END_OF_TURN_TOKEN|>"
  }
]